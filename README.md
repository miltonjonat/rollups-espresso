# rollups-espresso

Experimentation to allow the creation of Cartesi DApps using Espresso's sequencer and DA

## Rationale

[Espresso Systems](https://www.espressosys.com/) provides a decentralized sequencer and data availability system, which can be of use to scale layer-2 rollup solutions.
[Cartesi](https://cartesi.io) provides a [rollups solution](https://docs.cartesi.io/cartesi-rollups/) which could particularly benefit from both.

In this experiment, we do a minimalistic experimentation of using Espresso solely as a DA provider for Cartesi DApps. This will **not** provide much benefits in terms of transaction throughput, but it will allow larger volumes of data to be used in Cartesi DApps. In other words, it does not help applications that need clients to send hundreds of transactions, but it does help applications where larger sets of data are required.

## Architecture

The general procedure envisioned here to use Espresso data in Cartesi DApps is the following:

1. Client submits data to Espresso, using a VM ID derived from the Cartesi DApp's address
1. Client waits for the data to be included in an Espresso block, and notes the block's hash
1. Client submits the block hash to an `EspressoRelay` contract on the base layer, so that it is sent to the Cartesi DApp's back-end
1. The `EspressoRelay` checks with Espresso's `History` contract if the block hash is indeed valid
1. The `EspressoRelay` adds the block hash as an input to the Cartesi DApp
1. The Cartesi Node adds the input to be processed by the DApp back-end inside the Cartesi Machine, as with any other input
1. The DApp back-end recognizes that the input comes from the `EspressoRelay` contract, which it trusts to have verified that it is a valid Espresso block hash
1. The DApp back-end requests the corresponding block data using the `dehashing device`
1. The `dehashing device` fetches the specified block, and serves only the data associated with the Cartesi DApp's VM ID (derived from its address)
1. The DApp back-end receives the data and resumes processing

In this experiment, no thought is given to on-chain dispute resolution. It is assumed that this will be resolved later on by providing some sort of proof (maybe ZK) to associate an Espresso block hash with the Cartesi merkle tree root hash of its corresponding data.

## Components

To implement the proposed architecture, this repository contains the following components:

- `dehashing-server`: a simple HTTP server that receives requests for dehashing Espresso blocks, returning the corresponding data. This component is expected to run inside the Cartesi Node
- `onchain > EspressoRelay`: a smart contract for safely relaying Espresso block hashes as inputs to Cartesi DApps
- `echo-espresso`: a back-end that simply receives an input from the EspressoRelay and outputs a notice with the corresponding data

## Running

To run this experiment, perform the following steps:

1. Start a local sunodo development node without a back-end (aka run it in "host mode")

```shell
sunodo run --no-backend
```

2. Start the `dehashing-server` (it will run on port 5006)

```shell
cd dehashing-server
yarn && yarn start
```

3. In a separate terminal, run the `echo-espresso` back-end in the host

```shell
cd echo-espresso
yarn && yarn start
```

4. In a separate terminal, deploy the `EspressoRelay` contract to sunodo's internal Anvil instance:

```shell
cd onchain
yarn && yarn deploy
```

5. Send an Espresso block hash as input to the `EspressoRelay` contract

```shell
cast send 0x1fA2e8678b9EAE6048E546Be1B34a943670CF1ab \
  "relayBlock(address,bytes)(bytes32)" \
  "0x70ac08179605AF2D9e75782b8DEcDD3c22aA4D0C" \
  "0x424c4f434b7e78614c4151414a4e4448646342546e65464e333437596d72396461626b4768477661685934484e73564c2d61" \
  --mnemonic "test test test test test test test test test test test junk" \
  --mnemonic-index 0 \
  --rpc-url "http://localhost:8545"
```
(this will send block hash `BLOCK~xaLAQAJNDHdcBTneFN347Ymr9dabkGhGvahY4HNsVL-a`)

6. Check the corresponding output notice generated by the DApp back-end

```shell
curl http://localhost:8080/graphql -H 'Content-Type: application/json' -d '{"query":"{ notices { edges { node { payload } } } }"}'
```

### Submitting data to Espresso

To actually submit your own data to Espresso, these are the steps:

1. Check the current block height at Espresso (we are using the Cortado testnet in this experiment)

```shell
export ESPRESSO_BLOCK_HEIGHT=$(curl https://espresso.tspre.org/status/latest_block_height)
```

2. Use the returned block height to subscribe to Espresso's websocket, using grep to filter only the data that we will submit (here we assume VM ID `8118873125636387000`). 

```shell
npx wscat --connect wss://espresso.tspre.org/availability/stream/blocks/$ESPRESSO_BLOCK_HEIGHT | grep 8118873125636387000
```

3. In a separate terminal, send some arbitrary data to Espresso. For instance:

```shell
curl -X POST -H "Content-Type: application/json" -d '{ "vm": 8118873125636387000, "payload": [100, 110, 120, 130] }' https://espresso.tspre.org/submit/submit
```

4. Check the websocket stream to extract an appropriate block hash. The websocket will include several blocks for the same data, that is expected.
